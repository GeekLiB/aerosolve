# Example of using the generic pipeline for multiclass classification.
#
# Uses 20 newsgroups dataset (http://qwone.com/~jason/20Newsgroups/) as input. The training
# creates a model which can be used to predict which newsgroup a message came from.
#

job_name : "Generic Pipeline Multiclass"

# Settings as to where the data goes
training_data_version : 1
model_version: "a"

model_type : "full_rank_linear"
model_config : ${model_type}"_model_config"
demo_table : "pricing.twenty_news_training_raw"
# Can use a different table
demo_table_eval : "pricing.twenty_news_training_raw"

# Where to dump outputs; change prefix based on your HDFS directory layout
prefix : "hdfs://airfs-silver/user/"${USER}"/multiclass_demo_pipeline"
training_data : ${prefix}"/training_data"${training_data_version}
eval_data : ${prefix}"/eval_data"${training_data_version}

eval_output : ${prefix}"/eval_output/"${training_data_version}_${model_version}".eval"
model_name : ${prefix}"/model/"${training_data_version}_${model_version}_${model_type}".model"
model_dump : ${model_name}".tsv"
scoring_output : ${prefix}"/scores/"${training_data_version}

train_subsample : 1.0
eval_subsample : 1.0

# LABEL should be string in format [class1]:[weight1],[class2]:[weight2],....
#
# Use RAW name for string features that will be tokenized to prevent the feature name from
# being appended in.
generic_hive_query : """
  select
    concat(group_name, ":1.0") as LABEL,
    message as S_RAW
"""

debug_example {
  hive_query : ${generic_hive_query}" from "${demo_table}
  is_multiclass : true
  count : 10
}

debug_transforms {
  hive_query : ${generic_hive_query}" from "${demo_table}
  model_config : ${model_config}
  is_multiclass : true
  count : 10
}

make_training {
  training_hive_query : ${generic_hive_query}" from "${demo_table}
  training_output : ${training_data}

  # Eval data is optional
  eval_hive_query : ${generic_hive_query}" from "${demo_table_eval}
  eval_output : ${eval_data}

  is_multiclass : true
  num_shards : 20
}

train_model {
  input : ${training_data}
  subsample : ${train_subsample}
  model_config : ${model_config}
}

eval_model {
  input : ${eval_data}
  subsample : ${eval_subsample}
  bins : 11
  model_config : ${model_config}
  is_probability : false
  is_multiclass : true
  metric_to_maximize : "!HOLD_F1"
  model_name : ${model_name}
}

score_table {
  model_config : ${model_config}
  model_name : ${model_name}
  hive_query : ${generic_hive_query}", cast(id as string) as UNIQUE_ID from "${demo_table}
  is_multiclass : true
  num_shards : 20
  output : ${scoring_output}
}

debug_score_table {
  model_config : ${model_config}
  model_name : ${model_name}
  hive_query : ${generic_hive_query}", cast(id as string) as UNIQUE_ID from "${demo_table}
  is_multiclass : true
  count : 10
}

dump_full_rank_linear_model {
  model_name : ${model_name}
  model_dump : ${model_dump}

  # The number of features (sorted by weight, descending) to output for each label
  features_per_label : 50
}

identity_transform {
  transform : list
  transforms : [ ]
}

tokenize_string {
  transform : default_string_tokenizer
  field1: "S"
  regex : """[\s\p{Punct}]"""
  output : "TOKENS"
  generate_bigrams : false
}

normalize_tokens {
  transform : normalize_float
  field1: "TOKENS"
}

delete_string {
  transform : delete_string_feature_column
  field1: "S"
}

combined_transform {
  transform : list
  transforms : [
    tokenize_string,
    normalize_tokens,
    delete_string
  ]
}

# Config for a full rank linear model.
full_rank_linear_model_config {
  trainer : "full_rank_linear"
  model_output : ${model_name}
  rank_key : "LABEL"
  # Can be either "softmax" or "hinge"
  loss : "hinge"
  cache : "memory"
  subsample : 1.0
  iterations : 20
  lambda : 1.0
  min_count : 3
  context_transform : identity_transform
  item_transform : identity_transform
  combined_transform : combined_transform
}